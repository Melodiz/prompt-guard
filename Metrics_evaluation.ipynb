{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('server_test/data/result_with_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'target' column exists in the DataFrame with true labels\n",
    "# Map the predicted labels to numerical values\n",
    "df['Predicted'] = df['Label'].map({'INJECTION': 1, 'SAFE': 0})\n",
    "print(df.columns)\n",
    "# Calculate accuracy\n",
    "correct_predictions = (df['Predicted'] == df['target']).sum()\n",
    "total_predictions = len(df)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision, recall, and F1 score for each class\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(df['target'], df['Predicted'], labels=[1, 0])\n",
    "\n",
    "print(f\"Precision for INJECTION: {precision[0]:.2f}\")\n",
    "print(f\"Recall for INJECTION: {recall[0]:.2f}\")\n",
    "print(f\"F1 Score for INJECTION: {f1[0]:.2f}\")\n",
    "\n",
    "print(f\"Precision for SAFE: {precision[1]:.2f}\")\n",
    "print(f\"Recall for SAFE: {recall[1]:.2f}\")\n",
    "print(f\"F1 Score for SAFE: {f1[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = df.isna().sum()\n",
    "\n",
    "print(\"Number of NaN values in each column:\")\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_sources = df['source'].unique()\n",
    "\n",
    "# Create a dictionary to hold DataFrames for each source\n",
    "source_dataframes = {}\n",
    "\n",
    "for source in unique_sources:\n",
    "    # Filter the DataFrame for the current source\n",
    "    source_df = df[df['source'] == source]\n",
    "    source_dataframes[source] = source_df\n",
    "\n",
    "    # Map the predicted labels to numerical values for the current source\n",
    "    # Calculate precision, recall, and F1 score for the current source\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(source_df['source'] == source, source_df['Predicted'], labels=[1, 0])\n",
    "\n",
    "    print(f\"Precision for {source}: {precision[0]:.2f}\")\n",
    "    print(f\"Recall for {source}: {recall[0]:.2f}\")\n",
    "    print(f\"F1 Score for {source}: {f1[0]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
